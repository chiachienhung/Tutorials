{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":" Exercise_3_modelopt.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QdTyBQIWFUKb","colab_type":"text"},"source":["## 3. Model Complexity\n","\n","Many machine learning models have parameters that control the complexity of the model. We call these **hyperparameters**.\n","\n","These hyperparameters have to be set in advance (i.e. before model training).\n","\n","There are 2 extreme scenarios:\n","\n","1. If the model is too **simple**, it will not be able to fit the training data well and it will perform poorly. It is called **underfitting**.\n","2. If the model is too **complex**, it will fit the training data (too) well and but perform poorly on unseen test data. It is called **overfitting**.\n","\n","![](https://miro.medium.com/max/555/1*tBErXYVvTw2jSUYK7thU2A.png)\n","\n","Therefore, it is required to optimize the hyperparameters, which is called **model selection**.\n","\n","If there are many hyperparameters, it is required to do model selection automatically."]},{"cell_type":"code","metadata":{"id":"Am_VuehjDcNF","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import operator\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.neighbors import KNeighborsClassifier"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qq7tBg-2FUKc","colab_type":"text"},"source":["### Exercise 3.0: Run a Polynomial regression model on the datapoints\n","\n","1. Generate random dataset (this has been done!)\n","\n","2. Do train and test split with test_size = 0.2, random_state = 42 by using the train_test_split() function from sklearn. Split x and y into x_train, x_test, y_train, y_test\n","\n","3. Plot the figure to see the distribution of data\n","\n","4. Use LinearRegression model and check the training and testing loss (i.e. under fitting)\n"," \n","5. Use Polynomial Regression model with lower degree and check the training and testing loss (i.e. perfect fitting)\n","\n","6. Use Polynomial Regression model with higher degree and check the training and testing loss (i.e. over fitting)"]},{"cell_type":"code","metadata":{"id":"gl3m1YAdCvTX","colab_type":"code","colab":{}},"source":["# Generate random dataset\n","np.random.seed(0)\n","n_samples = 100\n","x = np.linspace(0, 10, n_samples)\n","y = x ** 3 + np.random.randn(n_samples) * 200 + 100\n","\n","### Do train and test split with test_size = 0.2, random_state = 42 by using the train_test_split() function from sklearn\n","x_train, x_test, y_train, y_test = \n","\n","# Plot the figure to see the results\n","plt.figure(figsize=(6,4))\n","plt.scatter(x_train, y_train, s=10, label='train')\n","plt.scatter(x_test, y_test, s=10, label = 'test')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9ZgvVS4Cx5w","colab_type":"code","colab":{}},"source":["## Underfitting\n","\n","# Generate random dataset\n","np.random.seed(0)\n","n_samples = 100\n","x = np.linspace(0, 10, n_samples)\n","y = x ** 3 + np.random.randn(n_samples) * 200 + 100\n","\n","x = x[:, np.newaxis] #(100, ) to (100,1)\n","y = y[:, np.newaxis]\n","\n","### Do train and test split with test_size = 0.2, random_state = 42 by using the train_test_split() function from sklearn\n","x_train, x_test, y_train, y_test = \n","\n","### Fit Linear Regression model to the training set\n","model = \n","model.fit(  ,   )\n","\n","### Predict the learned model on training / testing set to identify the training / testing loss\n","y_pred_train = \n","y_pred_test = \n","\n","# Print the training / testing loss\n","print('Training loss:', np.sqrt(mean_squared_error(y_train,y_pred_train)))\n","print('Testing loss:', np.sqrt(mean_squared_error(y_test,y_pred_test)))\n","      \n","  \n","# Plot the figure concerning training / testing data distribution\n","plt.figure(figsize=(6,4))\n","plt.scatter(x_train, y_train, s=10, label='train')  \n","plt.scatter(x_test, y_test, s=10, label = 'test')\n","\n","# Sort the values of x before line plot\n","sort_axis = operator.itemgetter(0)\n","sorted_zip = sorted(zip(x_train,y_pred_train), key=sort_axis)\n","x_train, y_pred_train = zip(*sorted_zip)\n","\n","# Add the linear regression line as the learned model\n","plt.plot(x_train, y_pred_train, color='r')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xV0Y82QQKddB","colab_type":"code","colab":{}},"source":["## Perfect fitting with degree = 3\n","\n","# Generate random dataset\n","np.random.seed(0)\n","n_samples = 100\n","\n","x = np.linspace(0, 10, n_samples)\n","y = x ** 3 + np.random.randn(n_samples) * 200 + 100\n","\n","x = x[:, np.newaxis] #(100, ) to (100,1)\n","y = y[:, np.newaxis]\n","\n","### Do train and test split with test_size = 0.2, random_state = 42 by using the train_test_split() function from sklearn\n","x_train, x_test, y_train, y_test = \n","\n","### Fit Polynomial Regression model with degree=3 to the training set\n","polynomial_features= \n","x_poly_train = \n","x_poly_test = \n","\n","### Build the model\n","model = \n","model.fit(  ,   )\n","\n","### Predict the learned model on training / testing set to identify the training / testing loss\n","y_pred_train = \n","y_pred_test = \n","\n","# Print the training / testing loss\n","print('Training loss:', np.sqrt(mean_squared_error(y_train,y_pred_train)))\n","print('Testing loss:', np.sqrt(mean_squared_error(y_test,y_pred_test)))\n","  \n","# Plot the figure concerning training / testing data distribution\n","plt.figure(figsize=(6,4))\n","plt.scatter(x_train, y_train, s=10, label='train')  \n","plt.scatter(x_test, y_test, s=10, label = 'test')\n","\n","# Sort the values of x before line plot\n","sort_axis = operator.itemgetter(0)\n","sorted_zip = sorted(zip(x_train,y_pred_train), key=sort_axis)\n","x_train, y_pred_train = zip(*sorted_zip)\n","\n","# Add the polynomial regression line as the learned model\n","plt.plot(x_train, y_pred_train, color='r')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8q9zJmCPOeQo","colab_type":"code","colab":{}},"source":["## Over-fitting with degree = 15\n","\n","# Generate random dataset\n","np.random.seed(0)\n","n_samples = 100\n","\n","x = np.linspace(0, 10, n_samples)\n","y = x ** 3 + np.random.randn(n_samples) * 200 + 100\n","\n","x = x[:, np.newaxis] #(100, ) to (100,1)\n","y = y[:, np.newaxis]\n","\n","### Do train and test split with test_size = 0.2, random_state = 42 by using the train_test_split() function from sklearn\n","x_train, x_test, y_train, y_test = \n","\n","### Fit Polynomial Regression model with degree=15 to the training set\n","polynomial_features= \n","x_poly_train = \n","x_poly_test = \n","\n","### Build the model\n","model = \n","model.fit(, )\n","\n","### Predict the learned model on training / testing set to identify the training / testing loss\n","y_pred_train = \n","y_pred_test = \n","\n","# Print the training / testing loss\n","print('Training loss:', np.sqrt(mean_squared_error(y_train,y_pred_train)))\n","print('Testing loss:', np.sqrt(mean_squared_error(y_test,y_pred_test)))\n","\n","# Plot the figure concerning training / testing data distribution\n","plt.figure(figsize=(6,4))      \n","plt.scatter(x_train, y_train, s=10, label='train')  \n","plt.scatter(x_test, y_test, s=10, label = 'test')\n","\n","# sort the values of x before line plot\n","sort_axis = operator.itemgetter(0)\n","sorted_zip = sorted(zip(x_train,y_pred_train), key=sort_axis)\n","x_train, y_pred_train = zip(*sorted_zip)\n","\n","# Add the polynomial regression line as the learned model\n","plt.plot(x_train, y_pred_train, color='r')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6NfCLtnCFUKh","colab_type":"text"},"source":["### 3.1 Optimize Hyperparameters\n","\n","To optimize the hyperparameters, we train the model with various hyperparameter values, and test what model performs best\n","\n","### 3.1.1 Fixed-split cross-validation\n","1. A dataset is partitioned into the training, validation, and testing sets. \n","2. The training set is used to fit a model by giving a set of hyperparameters\n","3. The validation set is used to evaluate the performance of the model given the hyperparameters. \n","4. We repeat step 2 and 3 by issuing different sets of hyperparameters and pick the set that leads to the highest validation\n","5. Use both the training and validation sets to train our final model, and apply it to the testing set to evaluate the model performance. \n","\n","The following figure illustrates the procedure:\n","\n","![](https://i.stack.imgur.com/osBuF.png)\n","\n","One major disadvantage of the fixed-split method is that the validation and testing performance is sensitive to the random\n","splits. If we have a unfortunate split such that the validation (resp. testing) set is unrepresentative, we may end up picking\n","suboptimal hyperparameters (resp. reporting a misleading performance score)."]},{"cell_type":"markdown","metadata":{"id":"DRLgC5POFUKi","colab_type":"text"},"source":["### 3.1.2 Folded (K-fold) Cross-Validation\n","Next, we take a look at a more robust technique called the K-Fold Cross-Validation.\n","\n","In K-fold cross-validation (CV), we randomly split the training dataset into folds without replacement, where folds\n","are used for the model training and the remaining 1 fold is for testing. This procedure is repeated times so that we obtain\n","models and performance estimates. Then we take their average as the final performance estimate. The following\n","figure illustrate the 10-fold CV:\n","![](http://karlrosaen.com/ml/learning-log/2016-06-20/k-fold-diagram.png)\n","\n","The advantage of this\n","approach is that the performance is less sensitive to unfortunate splits of data. In addition, it utilize data better since each\n","example can be used for both training and validation/testing."]},{"cell_type":"markdown","metadata":{"id":"7Ek5XbZTFUKj","colab_type":"text"},"source":["## Fixed-split vs K-fold Cross-Validation\n","\n","Cross-validation is usually the preferred method because it gives your model the opportunity to train on multiple train-test splits. \n","\n","This gives you a better indication of how well your model will perform on unseen data. Fixed-split, on the other hand, is dependent on just one train-test split. \n","\n","That makes the fixed-split method score dependent on how the data is split into train and test sets.\n","\n","The hold-out method is good to use when you have a very large dataset, you’re on a time crunch, \n","or you are starting to build an initial model in your data science project. \n","\n","Keep in mind that because cross-validation uses multiple train-test splits, it takes more computational power and time to run than using the fixed-split method."]},{"cell_type":"markdown","metadata":{"id":"zDkfuvSDFUKk","colab_type":"text"},"source":["### Exercise 3.1: Fixed-split vs K-fold Cross-Validation\n","\n","1. Load the iris dataset from sklearn\n","\n","2. Use Fixed-split cross-validation to pick the best-fit hyperparameter k for KNN classifier\n","\n","3. Use K-fold cross-validation to pick the best-fit hyperparameter k for KNN classifier\n","\n","4. Interpret the results"]},{"cell_type":"markdown","metadata":{"id":"EI_VWwWbzcFu","colab_type":"text"},"source":["## What is KNN?\n","\n","KNN, standing for (K-nearest neighbor), is an algorithm which can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. \n","It is commonly used for its easy of interpretation and low calculation time, so can be implemented as a baseline method to compare.\n","\n","Suppose you have a data distributed like this with two different classes\n","![](https://i0.wp.com/www.analyticsvidhya.com/wp-content/uploads/2014/10/scenario1.png?w=715&ssl=1)\n","\n","\n","You intend to find out the class of the blue star (BS) . BS can either be RC or GS and nothing else. The “K” is KNN algorithm is the nearest neighbors we wish to take vote from. Let’s say K = 3. Hence, we will now make a circle with BS as center just as big as to enclose only three datapoints on the plane. Refer to following diagram for more details:\n","![](https://i2.wp.com/www.analyticsvidhya.com/wp-content/uploads/2014/10/scenario2.png?w=715&ssl=1)\n","\n","\n","The three closest points to BS is all RC. Hence, with good confidence level we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. "]},{"cell_type":"markdown","metadata":{"id":"nuWX7awsqnS5","colab_type":"text"},"source":["#### Fixed-split Cross-Validation"]},{"cell_type":"code","metadata":{"id":"8eBYPL90fHkR","colab_type":"code","colab":{}},"source":["# Load the iris dataset from sklearn\n","\n","iris = datasets.load_iris()\n","X, y = iris.data[:, [1, 2]], iris.target\n","\n","### fixed-split training / testing set\n","X_train, X_test, y_train, y_test = \n","\n","### fixed-split training / validation set\n","X_itrain, X_val, y_itrain, y_val = "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"arko7eMhfLiJ","colab_type":"code","colab":{}},"source":["best_k, best_score = -1, -1\n","clfs = {}\n","\n","# Hyperparameter tuning for K = [1, 15, 20]\n","for k in [1, 15, 50]:\n","  pipe = Pipeline([['sc', StandardScaler()],\n","                   ['clf', KNeighborsClassifier(n_neighbors=k)]])\n","  \n","  ### Fit training set\n","  pipe.fit(   ,  )\n","  \n","  ### Evaluate on validation set\n","  y_pred = pipe.predict()\n","  \n","  ### Only one score for the validation accuracy\n","  score = accuracy_score( , ) \n","  \n","  # Print the validation accuracy\n","  print('[{}-NN]\\nValidation accuracy: {}'.format(k, score)) \n","  \n","  # Select the best K concerning the validation accuracy score\n","  if score > best_score:\n","    best_k, best_score = k, score\n","      \n","  clfs[k] = pipe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4otmcNazfiWy","colab_type":"code","colab":{}},"source":["# Use the trained model with the optimized parameter and train on whole training set (i.e. training + validation set)\n","clf = clfs[best_k]\n","\n","### Fit the model to the whole training set\n","clf.fit(, )\n","\n","### Predict on testing data and report the evaluation result\n","y_pred = clf.predict()\n","print('\\nTest accuracy: %.2f (n_neighbors=%d selected by the holdout method)' %\n","      (accuracy_score( , ), best_k))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AozIuvhjqr0x","colab_type":"text"},"source":["#### K-fold Cross-Validation"]},{"cell_type":"code","metadata":{"id":"HY9aWRkndTOA","colab_type":"code","colab":{}},"source":["# Load the iris dataset from sklearn\n","iris = datasets.load_iris()\n","X, y = iris.data[:, [1, 2]], iris.target\n","\n","### fixed-split training / testing set\n","X_train, X_test, y_train, y_test = "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0iDvhkCdfNh","colab_type":"code","colab":{}},"source":["best_k, best_score = -1, -1\n","clfs = {}\n","\n","# Hyperparameter tuning for K = [1, 15, 20]\n","for k in [1, 15, 20]:\n","  pipe = Pipeline([['sc', StandardScaler()],\n","                   ['clf', KNeighborsClassifier(n_neighbors=k)]])\n","  \n","  ### Fit training set\n","  pipe.fit(  ,   )\n","  \n","  ### K-Fold CV, evaluate the validation accuracy score for each fold\n","  scores = \n","  \n","  # Print the result of validation accuracy score\n","  print('[%d-NN]\\nValidation accuracy: %.3f %s' % (k, scores.mean(), scores)) \n","  \n","  # Select the best K concerning the validation accuracy score\n","  if scores.mean() > best_score:\n","    best_k, best_score = k, scores.mean()\n","    \n","  clfs[k] = pipe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kKYk1sqbeIt6","colab_type":"code","colab":{}},"source":["# Use the trained model with the optimized parameter and train on whole training set (i.e. training + validation set)\n","best_clf = clfs[best_k]\n","\n","### Fit the model to the whole training set\n","best_clf.fit( , )\n","\n","### Predict on testing data and report the evaluation result\n","y_pred = best_clf.predict(X_test)\n","print('\\nTest accuracy: %.2f (n_neighbors=%d selected by 5-fold CV)' %\n","      (accuracy_score(   ,  ), best_k))"],"execution_count":0,"outputs":[]}]}